{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/molly/anaconda3/lib/python3.7/site-packages/smart_open/ssh.py:34: UserWarning: paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "  warnings.warn('paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import empty\n",
    "import random\n",
    "import string\n",
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from nltk.corpus import stopwords\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in dictionary for mapping keywords to skills\n",
    "course = pd.read_csv('./data/courses_textrank_labels.csv')\n",
    "course['text'] = course['title'] + ' ' + course['description']\n",
    "label_to_skill = dict(zip(course['label'], course['skill']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trained NER model\n",
    "model_dir='./models/train_textrank_labels'\n",
    "nlp = spacy.load(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply trained NER\n",
    "def apply_model(test_text):\n",
    "    skills = []\n",
    "    doc = nlp(test_text)\n",
    "    for ent in doc.ents:\n",
    "        if label_to_skill.get(ent.text):\n",
    "            skills.append(label_to_skill.get(ent.text))\n",
    "        else:\n",
    "            skills.append(ent.text)\n",
    "    return skills"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "course['predicted'] = course['text'].map(apply_model)\n",
    "# combine multiple skills to single string\n",
    "course['predicted_str'] = course['predicted'].apply(lambda x: ' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare for preprocessing\n",
    "punctuations = string.punctuation\n",
    "stopwords = stopwords.words('english')\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize the strings\n",
    "def tokenize(doc):\n",
    "    doc = nlp(doc, disable=['parser', 'ner'])\n",
    "    tokens = [tok.lemma_.lower().strip() for tok in doc if tok.lemma_ != '-PRON-']\n",
    "    tokens = [tok for tok in tokens if tok not in stopwords and tok not in punctuations]\n",
    "    tokens = [tok.translate(str.maketrans('', '', string.punctuation)) for tok in tokens] # remove remaining punctuations\n",
    "    tokens = [''.join([i for i in tok if not i.isdigit()]) for tok in tokens]\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "course['predicted_tok'] = course['predicted_str'].map(tokenize)\n",
    "course['skill_tok'] = course['skill'].map(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrained word2vec embedding\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('lexvec.enwiki+newscrawl.300d.W.pos.vectors.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorize by word2vec, adding up vectors of each word to get synthetic vector for each skill  \n",
    "def vectorize(list):\n",
    "    vec_tot = empty([300,])\n",
    "    for tok in list:\n",
    "        try:\n",
    "            vec = model.get_vector(tok)\n",
    "            vec_tot += vec\n",
    "        except (KeyError):\n",
    "            continue\n",
    "    return vec_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "course['predicted_vec'] = course['predicted_tok'].apply(lambda x: vectorize(x))\n",
    "course['skill_vec'] = course['skill_tok'].apply(lambda x: vectorize(x))\n",
    "course['predicted_vec'] = course['predicted_vec'].apply(lambda x: np.nan_to_num(x))\n",
    "course['skill_vec'] = course['skill_vec'].apply(lambda x: np.nan_to_num(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate cosine similarity between predicted skill and true skill tag\n",
    "course['cos_sim'] = course.apply(lambda row: cosine_similarity([row['predicted_vec']], [row['skill_vec']]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.90272676]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "course['cos_sim'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.21077153846421604"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "course['cos_sim'].std()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
